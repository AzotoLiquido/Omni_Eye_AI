# ESEMPIO - Configurazione Omni Eye AI
# Copia questo file in .env e modifica i valori secondo le tue necessit√†

# ====================
# SICUREZZA
# ====================
DEBUG=False
SECRET_KEY=your-secret-key-here-generate-with-secrets-token-hex

# ====================
# SERVER
# ====================
HOST=127.0.0.1
PORT=5000

# ====================
# OLLAMA / AI
# ====================
# Per uso remoto (es. Termux), imposta l'IP del PC dove gira Ollama:
# OLLAMA_HOST=http://192.168.1.XX:11434
OLLAMA_HOST=http://localhost:11434
OLLAMA_MODEL=llama3.2
OLLAMA_TEMPERATURE=0.7
OLLAMA_MAX_TOKENS=2048
OLLAMA_CONTEXT_WINDOW=4096

# ====================
# UPLOAD FILES
# ====================
MAX_FILE_SIZE_MB=10
ALLOWED_EXTENSIONS=.txt,.pdf,.docx,.md,.py,.js,.html,.css,.json

# ====================
# RATE LIMITING
# ====================
# Protezione da abusi (loop infiniti, script massivi)
# I limiti sono MOLTO PERMISSIVI per non ostacolare il lavoro normale:
#   - 60 req/min per chat = 1 messaggio al secondo
#   - 20 req/min per upload = 1 file ogni 3 secondi  
#   - 120 req/min per operazioni conversazioni
#
# Se stai sviluppando intensamente, puoi disabilitare:
# RATE_LIMIT_ENABLED=False
RATE_LIMIT_ENABLED=True

# ====================
# LOGGING
# ====================
LOG_LEVEL=INFO
LOG_FILE=logs/omni_eye.log
